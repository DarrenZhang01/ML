{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DigitRecognizer_CNN(TF).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvaKh2g7s2Yf",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Image Classification \n",
        "Copyright: Zhibo Zhang, 2020\n",
        "\n",
        "[Link to the colab file](https://colab.research.google.com/drive/17wE_50l9h7LGsmr6OL3m5AeXMS8UVzmx?usp=sharing)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ild4h2TMqKYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsq9xYrThWbK",
        "colab_type": "text"
      },
      "source": [
        "Load the train and the test data in batches, and preprocess them\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TyxSBRUtBhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
        "\n",
        "train_x = train_x.reshape((60000, 28, 28, 1))\n",
        "test_x = test_x.reshape((10000, 28, 28, 1))\n",
        "\n",
        "train_x = tf.cast(train_x, tf.float32)\n",
        "test_x = tf.cast(test_x, tf.float32)\n",
        "train_y = tf.cast(train_y, tf.float32)\n",
        "test_y = tf.cast(test_y, tf.float32)\n",
        "\n",
        "train_y = to_categorical(train_y, 10)\n",
        "test_y = to_categorical(test_y, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VKCSVgztLhs",
        "colab_type": "text"
      },
      "source": [
        "Build the ResNet Model with CNN for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "og-Tmk23tTXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvModel, self).__init__()\n",
        "        self.Conv1 = layers.Conv2D(\n",
        "                filters=3, kernel_size=5, strides=(1, 1), \n",
        "                padding=\"same\")\n",
        "\n",
        "        self.Conv2 = layers.Conv2D(\n",
        "                filters=64, kernel_size=5, strides=(1, 1), \n",
        "                padding=\"same\", activation=\"relu\")\n",
        "        self.MaxPool2 = layers.MaxPool2D(pool_size=(2, 2))\n",
        "\n",
        "        self.Conv3 = layers.Conv2D(\n",
        "                filters=128, kernel_size=5, strides=(1, 1), \n",
        "                padding=\"same\", activation=\"relu\")\n",
        "        self.MaxPool3 = layers.MaxPool2D(pool_size=(2, 2))\n",
        "        self.batchnorm = layers.BatchNormalization()\n",
        "\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.fully_conn1 = layers.Dense(256, activation='relu')\n",
        "        self.fully_conn2 = layers.Dense(10, activation='sigmoid')\n",
        "\n",
        "    def call(self, x):       \n",
        "        block1 = self.Conv1(x)\n",
        "        block2 = self.MaxPool2(self.Conv2(block1))\n",
        "        block3 = self.batchnorm(self.MaxPool3(self.Conv3(block2)))\n",
        "        \n",
        "        flatten = self.flatten(block3)\n",
        "        fl1 = self.fully_conn1(flatten)\n",
        "        output = self.fully_conn2(fl1)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAwHAY1zq5U",
        "colab_type": "text"
      },
      "source": [
        "Define a loss (objective) function and the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dupjo6HYztWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "objective = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMLCdVBx3Eah",
        "colab_type": "text"
      },
      "source": [
        "Initialize and compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp2SQm7x3DSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = ConvModel()\n",
        "model.compile(optimizer=optimizer, loss=objective, metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaBUh6OjAERE",
        "colab_type": "text"
      },
      "source": [
        "Start training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9VDW51KAHAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "40d159f6-d75b-44c1-ae89-cf5c11d736bf"
      },
      "source": [
        "model.fit(\n",
        "    x=train_x,\n",
        "    y=train_y,\n",
        "    batch_size=128,\n",
        "    epochs=20\n",
        ")"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.1723 - accuracy: 0.9402\n",
            "Epoch 2/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0376 - accuracy: 0.9885\n",
            "Epoch 3/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0240 - accuracy: 0.9922\n",
            "Epoch 4/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0193 - accuracy: 0.9939\n",
            "Epoch 5/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0146 - accuracy: 0.9953\n",
            "Epoch 6/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0136 - accuracy: 0.9955\n",
            "Epoch 7/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0147 - accuracy: 0.9954\n",
            "Epoch 8/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0114 - accuracy: 0.9968\n",
            "Epoch 9/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0091 - accuracy: 0.9973\n",
            "Epoch 10/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0096 - accuracy: 0.9972\n",
            "Epoch 11/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0094 - accuracy: 0.9974\n",
            "Epoch 12/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0098 - accuracy: 0.9970\n",
            "Epoch 13/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0090 - accuracy: 0.9973\n",
            "Epoch 14/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0069 - accuracy: 0.9978\n",
            "Epoch 15/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0054 - accuracy: 0.9984\n",
            "Epoch 16/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0058 - accuracy: 0.9985\n",
            "Epoch 17/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0051 - accuracy: 0.9984\n",
            "Epoch 18/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0050 - accuracy: 0.9985\n",
            "Epoch 19/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0095 - accuracy: 0.9973\n",
            "Epoch 20/20\n",
            "469/469 [==============================] - 4s 8ms/step - loss: 0.0063 - accuracy: 0.9984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0174601160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLSJDvqxAvNk",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZYXpY5sA0Xw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ae475d7-a8f9-40e2-e4f9-8006e4cd58e0"
      },
      "source": [
        "loss, acc = model.evaluate(\n",
        "    x=test_x,\n",
        "    y=test_y,\n",
        "    batch_size=128\n",
        ")"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9929\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}